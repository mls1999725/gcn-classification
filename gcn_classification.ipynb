{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gcn-classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGpd3kVzi1wg",
        "colab_type": "code",
        "outputId": "b201bd81-5b59-4d46-cf72-a6561d098856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "\n",
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
        "                    enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
        "                             dtype=np.int32)\n",
        "    return labels_onehot\n",
        "\n",
        "def load_data(path=\"\", dataset=\"cora\"):\n",
        "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
        "                                        dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    #print(idx_features_labels.shape)\n",
        "    print(type(features))\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "    print(labels.shape)\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
        "                                    dtype=np.int32)\n",
        "    print(edges_unordered)\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
        "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
        "    print(edges.shape)\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                        shape=(labels.shape[0], labels.shape[0]),\n",
        "                        dtype=np.float32)\n",
        "\n",
        "    # build symmetric adjacency matrix\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    features = normalize(features)\n",
        "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "    idx_train = range(140)\n",
        "    idx_val = range(200, 500)\n",
        "    idx_test = range(500, 1500)\n",
        "\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test\n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n",
        "\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
        "#print(adj, features, labels, idx_train, idx_val, idx_test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cora dataset...\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "(2708, 7)\n",
            "[[     35    1033]\n",
            " [     35  103482]\n",
            " [     35  103515]\n",
            " ...\n",
            " [ 853118 1140289]\n",
            " [ 853155  853118]\n",
            " [ 954315 1155073]]\n",
            "(5429, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arvb4nLDAxub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from models import GCN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG5YbOf6n2Vf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9eb7363b-a215-4a95-9ccc-ffa91de975ef"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
        "\n",
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=16,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=0.5)\n",
        "lr = 0.01\n",
        "weight_decay = 5e-4\n",
        "optimizer = optim.Adam(model.parameters(),lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "# if args.cuda:\n",
        "#     model.cuda()\n",
        "#     features = features.cuda()\n",
        "#     adj = adj.cuda()\n",
        "#     labels = labels.cuda()\n",
        "#     idx_train = idx_train.cuda()\n",
        "#     idx_val = idx_val.cuda()\n",
        "#     idx_test = idx_test.cuda()\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(features, adj)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    fastmode = False\n",
        "    if not fastmode:\n",
        "        # Evaluate validation set performance separately,\n",
        "        # deactivates dropout during validation run.\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "epochs = 150\n",
        "for epoch in range(epochs):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Testing\n",
        "test()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cora dataset...\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "(2708, 7)\n",
            "[[     35    1033]\n",
            " [     35  103482]\n",
            " [     35  103515]\n",
            " ...\n",
            " [ 853118 1140289]\n",
            " [ 853155  853118]\n",
            " [ 954315 1155073]]\n",
            "(5429, 2)\n",
            "Epoch: 0001 loss_train: 2.0194 acc_train: 0.2071 loss_val: 2.0482 acc_val: 0.1567 time: 0.0259s\n",
            "Epoch: 0002 loss_train: 2.0054 acc_train: 0.2000 loss_val: 2.0360 acc_val: 0.1567 time: 0.0246s\n",
            "Epoch: 0003 loss_train: 1.9950 acc_train: 0.2000 loss_val: 2.0242 acc_val: 0.1567 time: 0.0259s\n",
            "Epoch: 0004 loss_train: 1.9805 acc_train: 0.2000 loss_val: 2.0127 acc_val: 0.1567 time: 0.0258s\n",
            "Epoch: 0005 loss_train: 1.9687 acc_train: 0.2000 loss_val: 2.0013 acc_val: 0.1567 time: 0.0300s\n",
            "Epoch: 0006 loss_train: 1.9641 acc_train: 0.2000 loss_val: 1.9901 acc_val: 0.1567 time: 0.0251s\n",
            "Epoch: 0007 loss_train: 1.9556 acc_train: 0.2000 loss_val: 1.9791 acc_val: 0.1567 time: 0.0253s\n",
            "Epoch: 0008 loss_train: 1.9360 acc_train: 0.1929 loss_val: 1.9682 acc_val: 0.1567 time: 0.0243s\n",
            "Epoch: 0009 loss_train: 1.9291 acc_train: 0.2000 loss_val: 1.9570 acc_val: 0.1567 time: 0.0245s\n",
            "Epoch: 0010 loss_train: 1.9197 acc_train: 0.2000 loss_val: 1.9455 acc_val: 0.1567 time: 0.0250s\n",
            "Epoch: 0011 loss_train: 1.8978 acc_train: 0.2071 loss_val: 1.9336 acc_val: 0.1567 time: 0.0255s\n",
            "Epoch: 0012 loss_train: 1.8949 acc_train: 0.2214 loss_val: 1.9213 acc_val: 0.1567 time: 0.0260s\n",
            "Epoch: 0013 loss_train: 1.8810 acc_train: 0.2071 loss_val: 1.9086 acc_val: 0.1567 time: 0.0304s\n",
            "Epoch: 0014 loss_train: 1.8920 acc_train: 0.1857 loss_val: 1.8958 acc_val: 0.1567 time: 0.0253s\n",
            "Epoch: 0015 loss_train: 1.8531 acc_train: 0.2000 loss_val: 1.8828 acc_val: 0.1567 time: 0.0290s\n",
            "Epoch: 0016 loss_train: 1.8399 acc_train: 0.2071 loss_val: 1.8697 acc_val: 0.1567 time: 0.0248s\n",
            "Epoch: 0017 loss_train: 1.8366 acc_train: 0.2071 loss_val: 1.8565 acc_val: 0.1567 time: 0.0244s\n",
            "Epoch: 0018 loss_train: 1.8041 acc_train: 0.2286 loss_val: 1.8431 acc_val: 0.1567 time: 0.0253s\n",
            "Epoch: 0019 loss_train: 1.8010 acc_train: 0.2214 loss_val: 1.8297 acc_val: 0.1567 time: 0.0251s\n",
            "Epoch: 0020 loss_train: 1.7839 acc_train: 0.2357 loss_val: 1.8164 acc_val: 0.1567 time: 0.0246s\n",
            "Epoch: 0021 loss_train: 1.7799 acc_train: 0.2429 loss_val: 1.8031 acc_val: 0.1733 time: 0.0271s\n",
            "Epoch: 0022 loss_train: 1.7575 acc_train: 0.2786 loss_val: 1.7898 acc_val: 0.3433 time: 0.0253s\n",
            "Epoch: 0023 loss_train: 1.7470 acc_train: 0.3571 loss_val: 1.7767 acc_val: 0.4367 time: 0.0264s\n",
            "Epoch: 0024 loss_train: 1.7479 acc_train: 0.4143 loss_val: 1.7642 acc_val: 0.4733 time: 0.0259s\n",
            "Epoch: 0025 loss_train: 1.7402 acc_train: 0.4429 loss_val: 1.7521 acc_val: 0.4100 time: 0.0276s\n",
            "Epoch: 0026 loss_train: 1.7158 acc_train: 0.4286 loss_val: 1.7406 acc_val: 0.3700 time: 0.0268s\n",
            "Epoch: 0027 loss_train: 1.7421 acc_train: 0.3214 loss_val: 1.7297 acc_val: 0.3633 time: 0.0254s\n",
            "Epoch: 0028 loss_train: 1.7005 acc_train: 0.3929 loss_val: 1.7194 acc_val: 0.3600 time: 0.0253s\n",
            "Epoch: 0029 loss_train: 1.6876 acc_train: 0.3500 loss_val: 1.7097 acc_val: 0.3533 time: 0.0268s\n",
            "Epoch: 0030 loss_train: 1.6943 acc_train: 0.2857 loss_val: 1.7005 acc_val: 0.3533 time: 0.0263s\n",
            "Epoch: 0031 loss_train: 1.6594 acc_train: 0.3429 loss_val: 1.6918 acc_val: 0.3533 time: 0.0261s\n",
            "Epoch: 0032 loss_train: 1.6592 acc_train: 0.3429 loss_val: 1.6835 acc_val: 0.3533 time: 0.0257s\n",
            "Epoch: 0033 loss_train: 1.6180 acc_train: 0.3357 loss_val: 1.6755 acc_val: 0.3533 time: 0.0255s\n",
            "Epoch: 0034 loss_train: 1.6485 acc_train: 0.3429 loss_val: 1.6676 acc_val: 0.3533 time: 0.0244s\n",
            "Epoch: 0035 loss_train: 1.6396 acc_train: 0.3143 loss_val: 1.6598 acc_val: 0.3600 time: 0.0269s\n",
            "Epoch: 0036 loss_train: 1.5953 acc_train: 0.3857 loss_val: 1.6520 acc_val: 0.3600 time: 0.0245s\n",
            "Epoch: 0037 loss_train: 1.6269 acc_train: 0.3571 loss_val: 1.6441 acc_val: 0.3633 time: 0.0290s\n",
            "Epoch: 0038 loss_train: 1.5911 acc_train: 0.3643 loss_val: 1.6361 acc_val: 0.3667 time: 0.0266s\n",
            "Epoch: 0039 loss_train: 1.5480 acc_train: 0.4143 loss_val: 1.6279 acc_val: 0.3700 time: 0.0261s\n",
            "Epoch: 0040 loss_train: 1.5774 acc_train: 0.3857 loss_val: 1.6197 acc_val: 0.3700 time: 0.0256s\n",
            "Epoch: 0041 loss_train: 1.5687 acc_train: 0.3786 loss_val: 1.6114 acc_val: 0.3800 time: 0.0247s\n",
            "Epoch: 0042 loss_train: 1.5466 acc_train: 0.4143 loss_val: 1.6031 acc_val: 0.3867 time: 0.0245s\n",
            "Epoch: 0043 loss_train: 1.5592 acc_train: 0.4143 loss_val: 1.5947 acc_val: 0.4033 time: 0.0250s\n",
            "Epoch: 0044 loss_train: 1.4873 acc_train: 0.4429 loss_val: 1.5859 acc_val: 0.4100 time: 0.0256s\n",
            "Epoch: 0045 loss_train: 1.4959 acc_train: 0.4571 loss_val: 1.5769 acc_val: 0.4133 time: 0.0280s\n",
            "Epoch: 0046 loss_train: 1.4790 acc_train: 0.4500 loss_val: 1.5678 acc_val: 0.4167 time: 0.0239s\n",
            "Epoch: 0047 loss_train: 1.4957 acc_train: 0.4571 loss_val: 1.5584 acc_val: 0.4200 time: 0.0247s\n",
            "Epoch: 0048 loss_train: 1.4256 acc_train: 0.4857 loss_val: 1.5486 acc_val: 0.4200 time: 0.0248s\n",
            "Epoch: 0049 loss_train: 1.4599 acc_train: 0.4714 loss_val: 1.5385 acc_val: 0.4367 time: 0.0239s\n",
            "Epoch: 0050 loss_train: 1.4497 acc_train: 0.4500 loss_val: 1.5281 acc_val: 0.4467 time: 0.0223s\n",
            "Epoch: 0051 loss_train: 1.4315 acc_train: 0.4857 loss_val: 1.5175 acc_val: 0.4533 time: 0.0274s\n",
            "Epoch: 0052 loss_train: 1.4139 acc_train: 0.4786 loss_val: 1.5066 acc_val: 0.4567 time: 0.0393s\n",
            "Epoch: 0053 loss_train: 1.3995 acc_train: 0.4929 loss_val: 1.4957 acc_val: 0.4600 time: 0.0283s\n",
            "Epoch: 0054 loss_train: 1.3550 acc_train: 0.4929 loss_val: 1.4845 acc_val: 0.4733 time: 0.0229s\n",
            "Epoch: 0055 loss_train: 1.3567 acc_train: 0.4929 loss_val: 1.4733 acc_val: 0.4767 time: 0.0279s\n",
            "Epoch: 0056 loss_train: 1.3547 acc_train: 0.4857 loss_val: 1.4622 acc_val: 0.4767 time: 0.0219s\n",
            "Epoch: 0057 loss_train: 1.3673 acc_train: 0.5000 loss_val: 1.4510 acc_val: 0.4767 time: 0.0232s\n",
            "Epoch: 0058 loss_train: 1.3089 acc_train: 0.5500 loss_val: 1.4398 acc_val: 0.4733 time: 0.0229s\n",
            "Epoch: 0059 loss_train: 1.3185 acc_train: 0.5214 loss_val: 1.4289 acc_val: 0.4767 time: 0.0243s\n",
            "Epoch: 0060 loss_train: 1.3256 acc_train: 0.4929 loss_val: 1.4182 acc_val: 0.4767 time: 0.0234s\n",
            "Epoch: 0061 loss_train: 1.2977 acc_train: 0.5071 loss_val: 1.4077 acc_val: 0.4800 time: 0.0243s\n",
            "Epoch: 0062 loss_train: 1.2832 acc_train: 0.4929 loss_val: 1.3973 acc_val: 0.4933 time: 0.0284s\n",
            "Epoch: 0063 loss_train: 1.2700 acc_train: 0.5071 loss_val: 1.3871 acc_val: 0.4967 time: 0.0238s\n",
            "Epoch: 0064 loss_train: 1.2703 acc_train: 0.5214 loss_val: 1.3770 acc_val: 0.5033 time: 0.0243s\n",
            "Epoch: 0065 loss_train: 1.2159 acc_train: 0.5357 loss_val: 1.3668 acc_val: 0.5000 time: 0.0247s\n",
            "Epoch: 0066 loss_train: 1.2287 acc_train: 0.5286 loss_val: 1.3566 acc_val: 0.5000 time: 0.0242s\n",
            "Epoch: 0067 loss_train: 1.2315 acc_train: 0.5714 loss_val: 1.3463 acc_val: 0.5000 time: 0.0246s\n",
            "Epoch: 0068 loss_train: 1.1688 acc_train: 0.5429 loss_val: 1.3362 acc_val: 0.5033 time: 0.0243s\n",
            "Epoch: 0069 loss_train: 1.1808 acc_train: 0.5286 loss_val: 1.3263 acc_val: 0.5167 time: 0.0241s\n",
            "Epoch: 0070 loss_train: 1.1705 acc_train: 0.5857 loss_val: 1.3169 acc_val: 0.5167 time: 0.0277s\n",
            "Epoch: 0071 loss_train: 1.1395 acc_train: 0.5786 loss_val: 1.3074 acc_val: 0.5167 time: 0.0287s\n",
            "Epoch: 0072 loss_train: 1.1636 acc_train: 0.5714 loss_val: 1.2979 acc_val: 0.5167 time: 0.0251s\n",
            "Epoch: 0073 loss_train: 1.1328 acc_train: 0.5643 loss_val: 1.2885 acc_val: 0.5233 time: 0.0261s\n",
            "Epoch: 0074 loss_train: 1.1444 acc_train: 0.5786 loss_val: 1.2792 acc_val: 0.5267 time: 0.0250s\n",
            "Epoch: 0075 loss_train: 1.1101 acc_train: 0.5857 loss_val: 1.2697 acc_val: 0.5333 time: 0.0235s\n",
            "Epoch: 0076 loss_train: 1.0727 acc_train: 0.6214 loss_val: 1.2606 acc_val: 0.5400 time: 0.0220s\n",
            "Epoch: 0077 loss_train: 1.1017 acc_train: 0.6000 loss_val: 1.2516 acc_val: 0.5567 time: 0.0217s\n",
            "Epoch: 0078 loss_train: 1.0990 acc_train: 0.6286 loss_val: 1.2428 acc_val: 0.5700 time: 0.0232s\n",
            "Epoch: 0079 loss_train: 1.0282 acc_train: 0.6429 loss_val: 1.2344 acc_val: 0.5867 time: 0.0255s\n",
            "Epoch: 0080 loss_train: 1.0677 acc_train: 0.6571 loss_val: 1.2260 acc_val: 0.6000 time: 0.0224s\n",
            "Epoch: 0081 loss_train: 1.0308 acc_train: 0.6500 loss_val: 1.2179 acc_val: 0.6067 time: 0.0223s\n",
            "Epoch: 0082 loss_train: 1.0367 acc_train: 0.6429 loss_val: 1.2101 acc_val: 0.6267 time: 0.0231s\n",
            "Epoch: 0083 loss_train: 1.0312 acc_train: 0.6786 loss_val: 1.2029 acc_val: 0.6500 time: 0.0233s\n",
            "Epoch: 0084 loss_train: 1.0181 acc_train: 0.7000 loss_val: 1.1962 acc_val: 0.6600 time: 0.0209s\n",
            "Epoch: 0085 loss_train: 1.0071 acc_train: 0.7643 loss_val: 1.1896 acc_val: 0.6633 time: 0.0222s\n",
            "Epoch: 0086 loss_train: 0.9883 acc_train: 0.7429 loss_val: 1.1824 acc_val: 0.6667 time: 0.0220s\n",
            "Epoch: 0087 loss_train: 0.9754 acc_train: 0.7429 loss_val: 1.1745 acc_val: 0.6700 time: 0.0250s\n",
            "Epoch: 0088 loss_train: 0.9957 acc_train: 0.7143 loss_val: 1.1664 acc_val: 0.6800 time: 0.0222s\n",
            "Epoch: 0089 loss_train: 0.9567 acc_train: 0.7357 loss_val: 1.1589 acc_val: 0.6767 time: 0.0222s\n",
            "Epoch: 0090 loss_train: 0.9802 acc_train: 0.7929 loss_val: 1.1515 acc_val: 0.6833 time: 0.0232s\n",
            "Epoch: 0091 loss_train: 0.9695 acc_train: 0.7500 loss_val: 1.1445 acc_val: 0.6867 time: 0.0277s\n",
            "Epoch: 0092 loss_train: 0.9732 acc_train: 0.7714 loss_val: 1.1374 acc_val: 0.6900 time: 0.0255s\n",
            "Epoch: 0093 loss_train: 0.9332 acc_train: 0.7714 loss_val: 1.1301 acc_val: 0.7000 time: 0.0228s\n",
            "Epoch: 0094 loss_train: 0.9365 acc_train: 0.7643 loss_val: 1.1232 acc_val: 0.7033 time: 0.0217s\n",
            "Epoch: 0095 loss_train: 0.9658 acc_train: 0.7714 loss_val: 1.1165 acc_val: 0.7033 time: 0.0221s\n",
            "Epoch: 0096 loss_train: 0.9184 acc_train: 0.7714 loss_val: 1.1099 acc_val: 0.7133 time: 0.0249s\n",
            "Epoch: 0097 loss_train: 0.9272 acc_train: 0.8071 loss_val: 1.1035 acc_val: 0.7200 time: 0.0222s\n",
            "Epoch: 0098 loss_train: 0.9261 acc_train: 0.8071 loss_val: 1.0971 acc_val: 0.7233 time: 0.0210s\n",
            "Epoch: 0099 loss_train: 0.9456 acc_train: 0.8286 loss_val: 1.0908 acc_val: 0.7233 time: 0.0220s\n",
            "Epoch: 0100 loss_train: 0.9033 acc_train: 0.8000 loss_val: 1.0844 acc_val: 0.7233 time: 0.0220s\n",
            "Epoch: 0101 loss_train: 0.8922 acc_train: 0.7857 loss_val: 1.0775 acc_val: 0.7267 time: 0.0212s\n",
            "Epoch: 0102 loss_train: 0.8918 acc_train: 0.8214 loss_val: 1.0706 acc_val: 0.7300 time: 0.0231s\n",
            "Epoch: 0103 loss_train: 0.8880 acc_train: 0.8000 loss_val: 1.0641 acc_val: 0.7300 time: 0.0238s\n",
            "Epoch: 0104 loss_train: 0.9098 acc_train: 0.8000 loss_val: 1.0578 acc_val: 0.7333 time: 0.0249s\n",
            "Epoch: 0105 loss_train: 0.8095 acc_train: 0.8357 loss_val: 1.0517 acc_val: 0.7400 time: 0.0256s\n",
            "Epoch: 0106 loss_train: 0.8444 acc_train: 0.8500 loss_val: 1.0454 acc_val: 0.7433 time: 0.0236s\n",
            "Epoch: 0107 loss_train: 0.8293 acc_train: 0.8429 loss_val: 1.0391 acc_val: 0.7433 time: 0.0222s\n",
            "Epoch: 0108 loss_train: 0.8196 acc_train: 0.8571 loss_val: 1.0327 acc_val: 0.7467 time: 0.0219s\n",
            "Epoch: 0109 loss_train: 0.8135 acc_train: 0.8500 loss_val: 1.0261 acc_val: 0.7533 time: 0.0237s\n",
            "Epoch: 0110 loss_train: 0.8380 acc_train: 0.8214 loss_val: 1.0193 acc_val: 0.7667 time: 0.0227s\n",
            "Epoch: 0111 loss_train: 0.8745 acc_train: 0.8143 loss_val: 1.0125 acc_val: 0.7700 time: 0.0237s\n",
            "Epoch: 0112 loss_train: 0.8106 acc_train: 0.8500 loss_val: 1.0062 acc_val: 0.7833 time: 0.0263s\n",
            "Epoch: 0113 loss_train: 0.7859 acc_train: 0.8643 loss_val: 0.9999 acc_val: 0.7867 time: 0.0278s\n",
            "Epoch: 0114 loss_train: 0.7762 acc_train: 0.8571 loss_val: 0.9928 acc_val: 0.7867 time: 0.0281s\n",
            "Epoch: 0115 loss_train: 0.7997 acc_train: 0.9000 loss_val: 0.9856 acc_val: 0.7900 time: 0.0262s\n",
            "Epoch: 0116 loss_train: 0.7776 acc_train: 0.8429 loss_val: 0.9788 acc_val: 0.7900 time: 0.0244s\n",
            "Epoch: 0117 loss_train: 0.7962 acc_train: 0.8429 loss_val: 0.9717 acc_val: 0.7967 time: 0.0231s\n",
            "Epoch: 0118 loss_train: 0.7691 acc_train: 0.8857 loss_val: 0.9646 acc_val: 0.8033 time: 0.0215s\n",
            "Epoch: 0119 loss_train: 0.7417 acc_train: 0.8929 loss_val: 0.9582 acc_val: 0.8033 time: 0.0211s\n",
            "Epoch: 0120 loss_train: 0.7108 acc_train: 0.9000 loss_val: 0.9513 acc_val: 0.8067 time: 0.0222s\n",
            "Epoch: 0121 loss_train: 0.7214 acc_train: 0.8857 loss_val: 0.9441 acc_val: 0.8067 time: 0.0216s\n",
            "Epoch: 0122 loss_train: 0.7234 acc_train: 0.8786 loss_val: 0.9368 acc_val: 0.8067 time: 0.0196s\n",
            "Epoch: 0123 loss_train: 0.7295 acc_train: 0.9214 loss_val: 0.9294 acc_val: 0.8167 time: 0.0222s\n",
            "Epoch: 0124 loss_train: 0.7212 acc_train: 0.8857 loss_val: 0.9216 acc_val: 0.8133 time: 0.0230s\n",
            "Epoch: 0125 loss_train: 0.7291 acc_train: 0.8857 loss_val: 0.9144 acc_val: 0.8133 time: 0.0266s\n",
            "Epoch: 0126 loss_train: 0.6946 acc_train: 0.8786 loss_val: 0.9079 acc_val: 0.8133 time: 0.0233s\n",
            "Epoch: 0127 loss_train: 0.6469 acc_train: 0.8857 loss_val: 0.9014 acc_val: 0.8133 time: 0.0217s\n",
            "Epoch: 0128 loss_train: 0.7024 acc_train: 0.8929 loss_val: 0.8945 acc_val: 0.8133 time: 0.0234s\n",
            "Epoch: 0129 loss_train: 0.6674 acc_train: 0.8786 loss_val: 0.8883 acc_val: 0.8133 time: 0.0237s\n",
            "Epoch: 0130 loss_train: 0.6416 acc_train: 0.9071 loss_val: 0.8830 acc_val: 0.8200 time: 0.0217s\n",
            "Epoch: 0131 loss_train: 0.6379 acc_train: 0.9071 loss_val: 0.8782 acc_val: 0.8267 time: 0.0222s\n",
            "Epoch: 0132 loss_train: 0.6868 acc_train: 0.8929 loss_val: 0.8730 acc_val: 0.8233 time: 0.0270s\n",
            "Epoch: 0133 loss_train: 0.6476 acc_train: 0.9000 loss_val: 0.8679 acc_val: 0.8233 time: 0.0228s\n",
            "Epoch: 0134 loss_train: 0.6363 acc_train: 0.9071 loss_val: 0.8624 acc_val: 0.8200 time: 0.0233s\n",
            "Epoch: 0135 loss_train: 0.6326 acc_train: 0.9071 loss_val: 0.8570 acc_val: 0.8133 time: 0.0243s\n",
            "Epoch: 0136 loss_train: 0.5955 acc_train: 0.8929 loss_val: 0.8518 acc_val: 0.8133 time: 0.0233s\n",
            "Epoch: 0137 loss_train: 0.6623 acc_train: 0.8643 loss_val: 0.8467 acc_val: 0.8133 time: 0.0244s\n",
            "Epoch: 0138 loss_train: 0.6301 acc_train: 0.9071 loss_val: 0.8420 acc_val: 0.8133 time: 0.0243s\n",
            "Epoch: 0139 loss_train: 0.5863 acc_train: 0.9071 loss_val: 0.8376 acc_val: 0.8100 time: 0.0232s\n",
            "Epoch: 0140 loss_train: 0.6211 acc_train: 0.8857 loss_val: 0.8336 acc_val: 0.8167 time: 0.0240s\n",
            "Epoch: 0141 loss_train: 0.6108 acc_train: 0.9143 loss_val: 0.8302 acc_val: 0.8167 time: 0.0277s\n",
            "Epoch: 0142 loss_train: 0.6369 acc_train: 0.8571 loss_val: 0.8271 acc_val: 0.8100 time: 0.0240s\n",
            "Epoch: 0143 loss_train: 0.6116 acc_train: 0.9357 loss_val: 0.8234 acc_val: 0.8100 time: 0.0230s\n",
            "Epoch: 0144 loss_train: 0.5919 acc_train: 0.9000 loss_val: 0.8201 acc_val: 0.8133 time: 0.0246s\n",
            "Epoch: 0145 loss_train: 0.5907 acc_train: 0.9071 loss_val: 0.8162 acc_val: 0.8100 time: 0.0267s\n",
            "Epoch: 0146 loss_train: 0.6042 acc_train: 0.9286 loss_val: 0.8119 acc_val: 0.8067 time: 0.0235s\n",
            "Epoch: 0147 loss_train: 0.5279 acc_train: 0.9357 loss_val: 0.8077 acc_val: 0.8067 time: 0.0244s\n",
            "Epoch: 0148 loss_train: 0.5961 acc_train: 0.9000 loss_val: 0.8037 acc_val: 0.8133 time: 0.0235s\n",
            "Epoch: 0149 loss_train: 0.5804 acc_train: 0.8929 loss_val: 0.8000 acc_val: 0.8233 time: 0.0242s\n",
            "Epoch: 0150 loss_train: 0.5563 acc_train: 0.9214 loss_val: 0.7969 acc_val: 0.8233 time: 0.0226s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 3.8174s\n",
            "Test set results: loss= 0.8709 accuracy= 0.8050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu9h_1ArqIcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}